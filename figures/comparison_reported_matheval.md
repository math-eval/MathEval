| Model                | MATH  | MATH-MathEval | GSM8k (5-shot) | GSM8K-MathEval |
|----------------------|-------|---------------|----------------|----------------|
| GPT-4                | 45.8  | 48.36         | 92             | 94.54          |
| GPT-3.5              | 28?   | 31.38         | 57.1           | 72.71          |
| LLaMA2-7B            | 2.5   | 5.76          | 14.6           | 17.74          |
| LLaMA2-7B-chat       | 3.9?  | 7.22          | 26.3?          | 26.84          |
| LLaMA2-13B           | 3.9   | 7.58          | 28.7           | 26.16          |
| LLaMA2-13B-chat      | 5.2   | 9.02          | 37.1           | 43.37          |
| LLaMA2-70B           | 13.5  | 15.22         | 56.8 (8-shot)  | 58.86          |
| LLaMA2-70B-chat      | 10.4  | 14.98         | 59.3           | 59.59          |
| chatGLMv2-6B         | 6.5   | 5.06          | 32.37          | 17.44          |
| Baichuan2-13B-base   | 10.08 | 12.4          | 52.77          | 53.9           |
| Qwen-14B             | 24.8 (4-shot) | 35.1 | 61.3 (8-shot)  | 62.77          |
| Qwen-14B-chat        | 18.4  | 42.72         | 60.1 (0) / 59.3 (8-shot) | 64.14 |
| MOSS-003-base-16B    | 2.4   | 3.26          | 6.9            | 7.88           |
| MammoTH_70B          | 41.8  | 21.84         | 76.9           | 71.19          |
| GAIRMath_Abel_70b    | 28.26 | 28.7          | 83.62          | 82.11          |
| InternLM-20B         | 7.9   | 16.62         | 52.6           | 46.1           |
| llemma_7b            | 18    | 17.06         | 36.4           | 36.01          |
| llemma_34b           | 25    | 24.52         | 51.5           | 51.48          |
| MetaMath-70B         | 26.6  | 27.52         | 82.3           | 77.56          |
